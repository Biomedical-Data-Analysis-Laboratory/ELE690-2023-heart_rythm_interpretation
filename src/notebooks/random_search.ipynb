{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from cardiac_rythm import preprocessing\n",
    "\n",
    "file_path = \"/home/halli/ux/BMDLab/matlab/resprog/GUI/CleanCutsDL/cutDataCinCTTI_rev_v2.mat\"\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "# Load data and create dataframe\n",
    "df = preprocessing.load_data(file_path)\n",
    "\n",
    "x = np.stack(df[\"s_ecg\"].to_numpy())\n",
    "x = x.reshape((*x.shape, 1))\n",
    "y = df[\"c_label\"].to_numpy()\n",
    "y = y - 1  # 0-4 instead of 1-5\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    test_size=0.1,\n",
    "    random_state=rng,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "# x_train, y_train = preprocessing.replicate_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from cardiac_rythm.models import CNN, CNNConfig\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    filters,\n",
    "    kernels,\n",
    "    strides,\n",
    "    pool,\n",
    "    fc_end,\n",
    "    dropout,\n",
    ") -> CNN:\n",
    "    model_config = CNNConfig(\n",
    "        filters,\n",
    "        kernels,\n",
    "        strides,\n",
    "        pool,\n",
    "        \"valid\",\n",
    "        fc_end,\n",
    "        dropout,\n",
    "    )\n",
    "    model = CNN(model_config)\n",
    "    opt = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        # weight_decay=0.001 / fit_settings.epochs,\n",
    "    )\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(\n",
    "    build_fn=get_model,\n",
    "    verbose=1,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    ")\n",
    "grid = dict(\n",
    "    filters=[[5, 10], [10, 5]],\n",
    "    kernels=[[5, 10], [10, 5]],\n",
    "    strides=[[1, 1], [2, 2]],\n",
    "    pool=[[0, 0], [[2, 2], [2, 2]]],\n",
    "    fc_end=[[32, 64], [64, 128]],\n",
    "    dropout=[0.3, 0.5],\n",
    ")\n",
    "\n",
    "\n",
    "searcher = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    n_jobs=2,\n",
    "    cv=3,\n",
    "    param_distributions=grid,\n",
    "    scoring=\"accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.fit(x_train, y_train, validation_data=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import RandomSearch, HyperParameters\n",
    "\n",
    "\n",
    "def build_model(hp: HyperParameters) -> CNN:\n",
    "    filters = []\n",
    "    kernels = []\n",
    "    strides = []\n",
    "    pool = []\n",
    "    for i in range(2):\n",
    "        filters.append(hp.Choice(f\"filters{i}\", [5, 10, 15, 20]))\n",
    "        kernels.append(hp.Choice(f\"kernels{i}\", [5, 10, 15, 20]))\n",
    "        strides.append(2)\n",
    "        pool.append([2, 2])\n",
    "\n",
    "    model_config = CNNConfig(\n",
    "        filters,\n",
    "        kernels,\n",
    "        strides,\n",
    "        pool,\n",
    "        \"valid\",\n",
    "        fc_end=[64, 128],\n",
    "        dropout=hp.Float(\"dropout\", 0.1, 0.9, step=0.1),\n",
    "    )\n",
    "    print(model_config)\n",
    "    model = CNN(model_config)\n",
    "    opt = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        # weight_decay=0.001 / fit_settings.epochs,\n",
    "    )\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "yt = pd.get_dummies(y_train)\n",
    "yv = pd.get_dummies(y_test)\n",
    "\n",
    "tuner = RandomSearch(build_model, objective=\"val_loss\", max_trials=5)\n",
    "tuner.search(\n",
    "    x_train,\n",
    "    yt,\n",
    "    validation_data=(x_test, yv),\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack(df[\"s_ecg\"].to_numpy())\n",
    "x = x.reshape((*x.shape, 1))\n",
    "y = df[\"c_label\"].to_numpy()\n",
    "y = y - 1  # 0-4 instead of 1-5\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    test_size=0.1,\n",
    "    random_state=rng,\n",
    "    stratify=y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.src.engine import tuner_utils\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from cardiac_rythm.visualize import visualize_history, visualize_test_result\n",
    "\n",
    "\n",
    "class RandomSearchOptimization(RandomSearch):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_folds: int = 10,\n",
    "        rng: np.random.RandomState = np.random.RandomState(0),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.skf = StratifiedKFold(n_splits=n_folds, random_state=rng, shuffle=True)\n",
    "\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        \"\"\"Override `run_trial` to implement cross validation.\"\"\"\n",
    "        X_train, y_train, X_test, y_test, *remaining_args = args\n",
    "\n",
    "        trial_dir = self.get_trial_dir(trial_id=trial.trial_id)\n",
    "        # Not using `ModelCheckpoint` to support MultiObjective.\n",
    "        # It can only track one of the metrics to save the best model.\n",
    "        saved_model = self._get_checkpoint_fname(trial.trial_id)\n",
    "        model_checkpoint = tuner_utils.SaveBestEpoch(\n",
    "            objective=self.oracle.objective,\n",
    "            filepath=saved_model,\n",
    "        )\n",
    "        original_callbacks = kwargs.pop(\"callbacks\", [])\n",
    "\n",
    "        # Run the training process multiple times.\n",
    "        histories = []\n",
    "        for execution in range(self.executions_per_trial):\n",
    "            total_acc = 0.0\n",
    "            total_val_acc = 0.0\n",
    "            total_loss = 0.0\n",
    "            total_val_loss = 0.0\n",
    "            for fold, (train_index, val_index) in enumerate(self.skf.split(X_train, y_train)):\n",
    "                xt, xv = X_train[train_index], X_train[val_index]\n",
    "                yt, yv = y_train[train_index], y_train[val_index]\n",
    "                yt = pd.get_dummies(yt)\n",
    "                yv = pd.get_dummies(yv)\n",
    "                copied_kwargs = copy.copy(kwargs)\n",
    "                callbacks = self._deepcopy_callbacks(original_callbacks)\n",
    "\n",
    "                # Add fold to tensorboard callback\n",
    "                for callback in callbacks:\n",
    "                    if isinstance(callback, tf.keras.callbacks.TensorBoard):\n",
    "                        callback.log_dir += f\"/fold_{fold}\"\n",
    "\n",
    "                self._configure_tensorboard_dir(callbacks, trial, execution)\n",
    "                callbacks.append(tuner_utils.TunerCallback(self, trial))\n",
    "                # Only checkpoint the best epoch across all executions.\n",
    "                callbacks.append(model_checkpoint)\n",
    "                copied_kwargs[\"callbacks\"] = callbacks\n",
    "\n",
    "                hp = trial.hyperparameters\n",
    "                model: CNN = self._try_build(hp)\n",
    "                history = self.hypermodel.fit(\n",
    "                    hp,\n",
    "                    model,\n",
    "                    xt,\n",
    "                    yt,\n",
    "                    validation_data=(xv, yv),\n",
    "                    *remaining_args,\n",
    "                    **copied_kwargs,\n",
    "                )\n",
    "\n",
    "                # Test the model and save conf mat, accuracy and loss\n",
    "                # TODO: Or just do it on the BEST model?\n",
    "                visualize_history(history.history, f\"{trial_dir}/{fold}.svg\")\n",
    "                # Dump history in case we want to plot it.\n",
    "                with open(f\"{trial_dir}/history_dict_{fold}\", \"wb\") as f:\n",
    "                    pickle.dump(history.history, f)\n",
    "                # Dump the model config in a better format\n",
    "                if fold == 0:\n",
    "                    with open(f\"{trial_dir}/model_config.json\", \"x\") as f:\n",
    "                        f.write(model.config.to_json(indent=2))\n",
    "\n",
    "                total_loss += min(history.history[\"loss\"])\n",
    "                total_val_loss += min(history.history[\"val_loss\"])\n",
    "                total_acc += max(history.history[\"accuracy\"])\n",
    "                total_val_acc += max(history.history[\"val_accuracy\"])\n",
    "\n",
    "            # Store average loss/accuracy\n",
    "            avg_loss = total_loss / self.skf.get_n_splits()\n",
    "            avg_val_loss = total_val_loss / self.skf.get_n_splits()\n",
    "            avg_accuracy = total_acc / self.skf.get_n_splits()\n",
    "            avg_val_accuracy = total_val_acc / self.skf.get_n_splits()\n",
    "\n",
    "            histories.append(\n",
    "                {\n",
    "                    \"loss\": avg_loss,\n",
    "                    \"val_loss\": avg_val_loss,\n",
    "                    \"accuracy\": avg_accuracy,\n",
    "                    \"val_accuracy\": avg_val_accuracy,\n",
    "                }\n",
    "            )\n",
    "            model.load_weights(saved_model)\n",
    "            prediction = np.argmax(model.predict(X_test), axis=1)\n",
    "            visualize_test_result(y_test, prediction, f\"{trial_dir}/conf_mat.svg\")\n",
    "\n",
    "        return histories\n",
    "\n",
    "\n",
    "class HyperTuner(HyperModel):\n",
    "    def __init__(self, n_filters: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_filters = n_filters\n",
    "\n",
    "    def build(self, hp: HyperParameters) -> CNN:\n",
    "        filters = []\n",
    "        kernels = []\n",
    "        strides = []\n",
    "        pool = []\n",
    "        for i in range(self.n_filters):\n",
    "            filters.append(hp.Choice(f\"filters{i}\", [5, 10, 15, 20]))\n",
    "            kernels.append(hp.Choice(f\"kernels{i}\", [5, 10, 15, 20]))\n",
    "            strides.append(2)\n",
    "            pool.append([2, 2])\n",
    "\n",
    "        model_config = CNNConfig(\n",
    "            filters,\n",
    "            kernels,\n",
    "            strides,\n",
    "            pool,\n",
    "            \"valid\",\n",
    "            fc_end=[64, 128],\n",
    "            dropout=hp.Float(\"dropout\", 0.1, 0.9, step=0.1),\n",
    "        )\n",
    "\n",
    "        model = CNN(model_config)\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.001,\n",
    "            # weight_decay=0.001 / fit_settings.epochs,\n",
    "        )\n",
    "        model.compile(\n",
    "            opt,\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "        patience=50,\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=\"/tmp/tb\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "tuner = RandomSearchOptimization(\n",
    "    n_folds=2,  # TODO: 10 10 10\n",
    "    hypermodel=HyperTuner(n_filters=2),\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=2,\n",
    "    # directory=f\"{i}\",\n",
    "    project_name=\"custom_training\",\n",
    "    overwrite=True,  # TODO: FALSE FALSE FALSE! (don't want to lose data after many trials..)\n",
    ")\n",
    "tuner.search(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test, \n",
    "    y_test,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from cardiac_rythm.hyper_tune import RandomSearchOptimization, HyperTuner\n",
    "n_filters = 2\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "        patience=50,\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=f\"results/{n_filters}/tb\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "tuner = RandomSearchOptimization(\n",
    "    rng,\n",
    "    n_folds=2,  # TODO: 10 10 10\n",
    "    hypermodel=HyperTuner(n_filters=n_filters),\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=2,\n",
    "    directory=\"results\",\n",
    "    project_name=f\"{n_filters=}\",\n",
    "    overwrite=True,  # TODO: FALSE FALSE FALSE! (don't want to lose data after many trials..)\n",
    ")\n",
    "tuner.search(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
